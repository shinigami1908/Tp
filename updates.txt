# src/features/feature_engineering.py
"""
Feature engineering / KPI computation (final fix).

This file addresses:
 - idp_goals overdue calculation (idp_goals__overdue_count)
 - avoiding "unknown" and "employee_id unknown" placeholder rows
 - preserving text concat columns (manager_comment_concat, comments_concat, summary_concat, areas_of_focus_concat)
 - merging employee role and org into final combined features
 - defensive checks for df not being DataFrame (avoids 'dict object has no attribute empty')
"""
from typing import Dict, Tuple, List, Optional, Any
import pandas as pd
import numpy as np
from ..common.logger import get_logger
import datetime
import re

log = get_logger("feature_engineering")

NUMERIC_HINTS = {
    "jira_metrics": ["story_points", "committed", "completed", "spillover", "bugs"],
    "github_metrics": ["commits", "pull_requests", "reviews_done", "copilot"],
    "defects": ["ppm", "rollbacks", "escaped_to_prod"],
    "rto": ["required_days", "in_office_days", "compliant"],
    "lms_completions": ["hours", "score"],
    "release_metrics": ["defect_rate_ppm", "rollbacks"],
    "idp_goals": [],
}

CATEGORICAL_EXPAND = ["severity", "sentiment", "status", "type", "rating", "bias_type"]


def _concat_texts(series: pd.Series, max_chars: int = 2000) -> str:
    try:
        vals = [str(x).strip() for x in series.dropna().astype(str)]
        seen = set()
        uniq = []
        for v in vals:
            if v and v not in seen:
                uniq.append(v)
                seen.add(v)
        joined = " || ".join(uniq)
        if len(joined) > max_chars:
            return joined[: max_chars - 3] + "..."
        return joined
    except Exception:
        return ""


def _normalize_loaded_keys(loaded: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    if not isinstance(loaded, dict):
        return loaded
    out = {}
    for k, v in loaded.items():
        nk = str(k).strip()
        nk = nk.replace(".csv", "").replace(".CSV", "")
        nk = nk.replace("-", "_").replace(" ", "_")
        while "__" in nk:
            nk = nk.replace("__", "_")
        nk = nk.lower()
        nk = nk.replace("feedback360", "feedback_360")
        nk = nk.replace("githubmetrics", "github_metrics")
        nk = nk.replace("lmscompletions", "lms_completions")
        nk = nk.replace("managerevaluations", "manager_evaluations")
        out[nk] = v
    return out


def _guess_numeric_cols(df: pd.DataFrame, hints: Optional[List[str]] = None) -> List[str]:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return []
    hints = hints or []
    cols = []
    for c in df.columns:
        if c in ("employee_id", "period_half"):
            continue
        if pd.api.types.is_numeric_dtype(df[c]):
            cols.append(c)
            continue
        name = c.lower()
        if any(h in name for h in hints):
            cols.append(c)
            continue
        try:
            sample = df[c].astype(str).str.strip().replace({"": None, "nan": None, "NA": None, "N/A": None}).dropna().head(40)
            if sample.empty:
                continue
            coerced = pd.to_numeric(sample, errors="coerce")
            if coerced.notna().mean() >= 0.6:
                cols.append(c)
        except Exception:
            continue
    return cols


def _safe_numeric_sum_agg(df: pd.DataFrame, group_cols: List[str], numeric_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty or not numeric_cols:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for c in numeric_cols:
        if c not in d.columns:
            continue
        try:
            d[c] = d[c].astype(str).str.strip().replace({"": None, "nan": None, "NA": None, "N/A": None})
            d[c] = pd.to_numeric(d[c], errors="coerce").fillna(0)
        except Exception:
            try:
                d[c] = pd.to_numeric(d[c], errors="coerce").fillna(0)
            except Exception:
                d[c] = 0
    agg = d.groupby(group_cols, dropna=False)[[c for c in numeric_cols if c in d.columns]].sum().reset_index()
    return agg


def _expand_categorical_counts(df: pd.DataFrame, group_cols: List[str], cat_cols: List[str]) -> pd.DataFrame:
    present = [c for c in cat_cols if isinstance(df, pd.DataFrame) and c in df.columns]
    if not present:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for c in present:
        try:
            d[c] = d[c].astype(str).str.strip().fillna("unknown").replace({"nan": "unknown", "None": "unknown"})
        except Exception:
            d[c] = d[c].fillna("unknown").astype(str)
    dummies = pd.get_dummies(d[present].apply(lambda s: s.astype(str)))
    combined = pd.concat([d[group_cols].reset_index(drop=True), dummies.reset_index(drop=True)], axis=1)
    agg = combined.groupby(group_cols, dropna=False).sum().reset_index()
    for col in agg.columns:
        if col not in group_cols:
            try:
                agg[col] = agg[col].astype(int)
            except Exception:
                agg[col] = pd.to_numeric(agg[col], errors="coerce").fillna(0).astype(int)
    return agg


def _base_row_counts(df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols + ["_rows"])
    g = df.groupby(group_cols, dropna=False).size().reset_index().rename(columns={0: "_rows"})
    return g


def _merge_prefix(df: pd.DataFrame, prefix: str, group_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols)
    out = df.copy()
    for c in df.columns:
        if c in group_cols:
            continue
        out = out.rename(columns={c: f"{prefix}__{c}"})
    return out


def _derive_safe_ratios(merged: pd.DataFrame) -> pd.DataFrame:
    df = merged.copy()
    def safe_div(a, b):
        try:
            a = float(a) if a not in (None, "") else 0.0
            b = float(b) if b not in (None, 0, "", "0") else 0.0
            if b == 0:
                return 0.0
            return a / b
        except Exception:
            return 0.0
    if "jira_metrics__story_points_completed" in df.columns and "jira_metrics__story_points_committed" in df.columns:
        df["completion_ratio"] = df.apply(lambda r: safe_div(r.get("jira_metrics__story_points_completed", 0),
                                                             r.get("jira_metrics__story_points_committed", 0)), axis=1)
    if "defects___rows" in df.columns and "defects__escaped_to_prod" in df.columns:
        df["defect_escape_rate"] = df.apply(lambda r: safe_div(r.get("defects__escaped_to_prod", 0),
                                                               r.get("defects___rows", 0)), axis=1)
    if "rto__compliant" in df.columns and "rto__required_days" in df.columns:
        df["rto_compliance_rate"] = df.apply(lambda r: safe_div(r.get("rto__compliant", 0),
                                                                r.get("rto__required_days", 0)), axis=1)
    if "github_metrics__copilot_suggestions_accepted" in df.columns and "github_metrics__copilot_suggestions_total" in df.columns:
        df["copilot_accept_ratio"] = df.apply(lambda r: safe_div(r.get("github_metrics__copilot_suggestions_accepted", 0),
                                                                  r.get("github_metrics__copilot_suggestions_total", 0)), axis=1)
    return df


def _process_text_fields_for_table(key: str, df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for gc in group_cols:
        if gc not in d.columns:
            d[gc] = "unknown"
    results = _base_row_counts(d, group_cols)
    results = _merge_prefix(results, key, group_cols)

    # feedback_360: sentiment counts + comments concat
    if key == "feedback_360":
        if "sentiment" in d.columns:
            sent_agg = _expand_categorical_counts(d, group_cols, ["sentiment"])
            sent_agg = _merge_prefix(sent_agg, key, group_cols)
            if not sent_agg.empty:
                results = results.merge(sent_agg, on=group_cols, how="left")
        if "comment" in d.columns:
            text_df = d.groupby(group_cols)["comment"].apply(lambda s: _concat_texts(s)).reset_index().rename(columns={"comment": "comments_concat"})
            text_df = _merge_prefix(text_df, key, group_cols)
            if not text_df.empty:
                results = results.merge(text_df, on=group_cols, how="left")

    # workday_checkins: type, sentiment, summary concat
    if key == "workday_checkins":
        if "type" in d.columns:
            type_agg = _expand_categorical_counts(d, group_cols, ["type"])
            type_agg = _merge_prefix(type_agg, key, group_cols)
            if not type_agg.empty:
                results = results.merge(type_agg, on=group_cols, how="left")
        if "sentiment" in d.columns:
            sent_agg = _expand_categorical_counts(d, group_cols, ["sentiment"])
            sent_agg = _merge_prefix(sent_agg, key, group_cols)
            if not sent_agg.empty:
                results = results.merge(sent_agg, on=group_cols, how="left")
        if "summary" in d.columns:
            text_df = d.groupby(group_cols)["summary"].apply(lambda s: _concat_texts(s)).reset_index().rename(columns={"summary": "summary_concat"})
            text_df = _merge_prefix(text_df, key, group_cols)
            if not text_df.empty:
                results = results.merge(text_df, on=group_cols, how="left")

    # manager_evaluations: manager_comment + areas_of_focus concat
    if key == "manager_evaluations":
        if "manager_comment" in d.columns:
            text_df = d.groupby(group_cols)["manager_comment"].apply(lambda s: _concat_texts(s)).reset_index().rename(columns={"manager_comment": "manager_comment_concat"})
            text_df = _merge_prefix(text_df, key, group_cols)
            if not text_df.empty:
                results = results.merge(text_df, on=group_cols, how="left")
        if "areas_of_focus" in d.columns:
            txt2 = d.groupby(group_cols)["areas_of_focus"].apply(lambda s: _concat_texts(s)).reset_index().rename(columns={"areas_of_focus": "areas_of_focus_concat"})
            txt2 = _merge_prefix(txt2, key, group_cols)
            if not txt2.empty:
                results = results.merge(txt2, on=group_cols, how="left")

    # Do not coerce text columns to numeric here; fill numeric columns below
    return results


def compute_kpis_from_loaded(loaded_raw: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if not isinstance(loaded_raw, dict):
        raise ValueError("compute_kpis_from_loaded expects a dict of DataFrames")
    loaded = _normalize_loaded_keys(loaded_raw)
    per_table_period: Dict[str, pd.DataFrame] = {}
    group_cols = ["employee_id", "period_half"]

    for key, df in loaded.items():
        try:
            # defensive checks
            if df is None or not isinstance(df, pd.DataFrame) or df.empty:
                per_table_period[key] = None
                continue

            working = df.copy()
            for gc in group_cols:
                if gc not in working.columns:
                    working[gc] = "unknown"

            # handle text-specialized tables first
            if key in ("feedback_360", "workday_checkins", "manager_evaluations"):
                txt_agg = _process_text_fields_for_table(key, working, group_cols)
                per_table_period[key] = txt_agg
                continue

            # IDP special handling: calculate overdue if not already present
            if key == "idp_goals":
                # ensure period_half exists already (periodizer sets NA when missing)
                if "period_half" not in working.columns:
                    working["period_half"] = "NA"
                # ensure status normalized
                if "status" not in working.columns:
                    working["status"] = ""
                # parse target_date and compute overdue flag if not present
                if "_idp_overdue_flag" not in working.columns:
                    def _is_overdue_row(r):
                        td_raw = r.get("target_date")
                        st = str(r.get("status", "")).strip().lower()
                        try:
                            td = pd.to_datetime(td_raw, errors="coerce", infer_datetime_format=True)
                        except Exception:
                            td = pd.NaT
                        if pd.isna(td):
                            return 0
                        if st == "completed":
                            return 0
                        # overdue if target_date < today
                        return int(td.date() < datetime.date.today())
                    working["_idp_overdue_flag"] = working.apply(_is_overdue_row, axis=1)
                # rows and status breakup
                rows_agg = _base_row_counts(working, group_cols)
                rows_agg = _merge_prefix(rows_agg, key, group_cols)
                status_agg = _expand_categorical_counts(working, group_cols, ["status"])
                status_agg = _merge_prefix(status_agg, key, group_cols) if not status_agg.empty else pd.DataFrame(columns=group_cols)
                overdue_agg = working.groupby(group_cols)["_idp_overdue_flag"].sum().reset_index().rename(columns={"_idp_overdue_flag": f"{key}__overdue_count"})
                merged = rows_agg
                if not status_agg.empty:
                    merged = merged.merge(status_agg, on=group_cols, how="left")
                if not overdue_agg.empty:
                    merged = merged.merge(overdue_agg, on=group_cols, how="left")
                # fill numeric nulls with 0; leave text columns as-is later
                for c in merged.columns:
                    if c in group_cols:
                        continue
                    try:
                        merged[c] = pd.to_numeric(merged[c], errors="coerce").fillna(0)
                    except Exception:
                        merged[c] = merged[c].fillna(0)
                per_table_period[key] = merged
                continue

            # generic numeric + categorical aggregation
            hints = NUMERIC_HINTS.get(key, [])
            numeric_candidates = _guess_numeric_cols(working, hints)
            numeric_agg = _safe_numeric_sum_agg(working, group_cols, numeric_candidates)
            numeric_agg = _merge_prefix(numeric_agg, key, group_cols) if not numeric_agg.empty else pd.DataFrame(columns=group_cols)
            cat_agg = _expand_categorical_counts(working, group_cols, CATEGORICAL_EXPAND)
            cat_agg = _merge_prefix(cat_agg, key, group_cols) if not cat_agg.empty else pd.DataFrame(columns=group_cols)
            rows_agg = _base_row_counts(working, group_cols)
            rows_agg = _merge_prefix(rows_agg, key, group_cols)
            merged = rows_agg
            if not numeric_agg.empty:
                merged = merged.merge(numeric_agg, on=group_cols, how="left")
            if not cat_agg.empty:
                merged = merged.merge(cat_agg, on=group_cols, how="left")
            per_table_period[key] = merged

        except Exception as e:
            log.exception("Error processing table %s: %s", key, e)
            per_table_period[key] = None

    # Build canonical key set BUT filter out missing/unknown employee_id and period_half == 'unknown'
    key_frames = []
    for k, agg_df in per_table_period.items():
        if agg_df is None or not isinstance(agg_df, pd.DataFrame) or agg_df.empty:
            continue
        if set(group_cols).issubset(set(agg_df.columns)):
            # filter rows with missing/empty employee_id or employee_id == 'unknown'
            tmp = agg_df[["employee_id", "period_half"]].drop_duplicates()
            tmp = tmp[tmp["employee_id"].notna()]
            # drop empty/blank or literal 'unknown'
            tmp = tmp[tmp["employee_id"].astype(str).str.strip().replace({"": np.nan}) != ""]
            tmp = tmp[tmp["employee_id"].astype(str).str.lower() != "unknown"]
            # drop rows where period_half is 'unknown' (we don't want those synthetic buckets)
            tmp = tmp[tmp["period_half"].astype(str).str.lower() != "unknown"]
            if not tmp.empty:
                key_frames.append(tmp)
    if key_frames:
        keys_df = pd.concat(key_frames, ignore_index=True).drop_duplicates().reset_index(drop=True)
    else:
        emp_df = loaded.get("employees")
        if isinstance(emp_df, pd.DataFrame) and "employee_id" in emp_df.columns:
            keys_df = emp_df[["employee_id"]].drop_duplicates().assign(period_half="NA")
        else:
            keys_df = pd.DataFrame(columns=group_cols)

    merged_all = keys_df.copy()

    # Left-merge each per-table aggregated df into merged_all
    for name, agg in per_table_period.items():
        if agg is None or not isinstance(agg, pd.DataFrame) or agg.empty:
            continue
        if not set(group_cols).issubset(set(agg.columns)):
            continue
        # Before merging, ensure agg has no rows with missing employee_id
        agg2 = agg.copy()
        agg2 = agg2[agg2["employee_id"].notna()]
        agg2 = agg2[agg2["employee_id"].astype(str).str.strip() != ""]
        agg2 = agg2[agg2["employee_id"].astype(str).str.lower() != "unknown"]
        # merge
        merged_all = merged_all.merge(agg2, on=group_cols, how="left")

    # Now: fill numeric columns with 0 and preserve text columns
    # Determine text-like columns by name patterns (concat, comment, message, summary, areas_of_focus, manager_comment)
    text_name_patterns = re.compile(r"(concat|comment|message|summary|areas_of_focus|comments_concat|manager_comment)", flags=re.IGNORECASE)
    for col in merged_all.columns:
        if col in group_cols:
            continue
        # treat as text column if name matches OR dtype is object and many values are non-numeric
        is_text_by_name = bool(text_name_patterns.search(col))
        if is_text_by_name:
            # fill strings
            merged_all[col] = merged_all[col].fillna("").astype(str)
            continue
        # else attempt numeric coercion: if most values are numeric or column name contains __ (prefixed metrics) we coerce
        try:
            coerced = pd.to_numeric(merged_all[col], errors="coerce")
            # if more than 50% non-null numeric, treat as numeric and fillna 0
            if coerced.notna().mean() >= 0.5:
                merged_all[col] = coerced.fillna(0)
            else:
                # treat as text: fill empty string
                merged_all[col] = merged_all[col].fillna("").astype(str)
        except Exception:
            merged_all[col] = merged_all[col].fillna("").astype(str)

    features_by_period_df = merged_all

    # Derived KPIs
    features_by_period_df = _derive_safe_ratios(features_by_period_df)

    # features_combined: aggregate numeric columns; keep text concats via first-non-empty
    if not features_by_period_df.empty:
        numeric_cols = [c for c in features_by_period_df.columns if c not in group_cols and merged_all[c].dtype.kind in "biufc"]
        # sum numeric cols
        features_combined_df = features_by_period_df.groupby("employee_id", dropna=False)[numeric_cols].sum().reset_index()
        # bring in text concat fields: pick first non-empty across periods
        text_cols = [c for c in features_by_period_df.columns if isinstance(c, str) and text_name_patterns.search(c)]
        for tc in text_cols:
            # create series per employee: join non-empty strings from periods
            combined_text = features_by_period_df.groupby("employee_id")[tc].apply(lambda s: _concat_texts(pd.Series([x for x in s if (isinstance(x, str) and x.strip() != "")]))).reset_index().rename(columns={tc: tc})
            features_combined_df = features_combined_df.merge(combined_text, on="employee_id", how="left")
        # recompute ratio KPIs at combined level
        features_combined_df = _derive_safe_ratios(features_combined_df)
    else:
        features_combined_df = pd.DataFrame(columns=["employee_id"])

    # Merge employee role/org (if available)
    emp_df = loaded.get("employees")
    if isinstance(emp_df, pd.DataFrame) and "employee_id" in emp_df.columns:
        # pick relevant columns
        pick = ["employee_id"]
        if "role" in emp_df.columns:
            pick.append("role")
        if "org" in emp_df.columns:
            pick.append("org")
        if "name" in emp_df.columns:
            pick.append("name")
        emp_meta = emp_df[pick].drop_duplicates(subset=["employee_id"])
        # merge into combined
        features_combined_df = features_combined_df.merge(emp_meta, on="employee_id", how="left")
    else:
        # nothing to merge
        pass

    log.info("compute_kpis_from_loaded: produced features_by_period rows=%s, features_combined rows=%s",
             len(features_by_period_df), len(features_combined_df))
    return features_combined_df, features_by_period_df


def compute_kpis(joined_or_loaded: Any) -> Tuple[pd.DataFrame, pd.DataFrame]:
    # if someone passed run_etl result with top-level keys, extract loaded
    if isinstance(joined_or_loaded, dict) and "loaded" in joined_or_loaded and not any(isinstance(v, pd.DataFrame) for v in joined_or_loaded.values()):
        loaded = joined_or_loaded.get("loaded")
        if loaded is None:
            return compute_kpis_from_loaded(joined_or_loaded)
        return compute_kpis_from_loaded(loaded)
    if isinstance(joined_or_loaded, dict):
        return compute_kpis_from_loaded(joined_or_loaded)
    if isinstance(joined_or_loaded, pd.DataFrame):
        # backward-compatible: compute simple numeric aggregation
        df = joined_or_loaded.copy()
        if "employee_id" not in df.columns:
            raise ValueError("compute_kpis received a DataFrame without 'employee_id' column")
        if "period_half" not in df.columns:
            if "period" in df.columns:
                df["period_half"] = df["period"].astype(str)
            else:
                df["period_half"] = "NA"
        group_cols = ["employee_id", "period_half"]
        numeric_candidates = []
        for c in df.columns:
            if c in ("employee_id", "period_half", "period", "name", "emp_role", "role"):
                continue
            try:
                sample = df[c].astype(str).str.strip().replace({"": None}).dropna().head(40)
                if sample.empty:
                    continue
                coerced = pd.to_numeric(sample, errors="coerce")
                if coerced.notna().mean() >= 0.6:
                    numeric_candidates.append(c)
            except Exception:
                continue
        if numeric_candidates:
            agg = df.groupby(group_cols, dropna=False)[numeric_candidates].sum().reset_index()
        else:
            agg = df.groupby(group_cols, dropna=False).size().reset_index().rename(columns={0: "_rows"})
        features_by_period_df = agg
        if not features_by_period_df.empty:
            numeric_cols = [c for c in features_by_period_df.columns if c not in group_cols]
            features_combined_df = features_by_period_df.groupby("employee_id", dropna=False)[numeric_cols].sum().reset_index()
        else:
            features_combined_df = pd.DataFrame(columns=["employee_id"])
        return features_combined_df, features_by_period_df
    raise ValueError("compute_kpis: unsupported input type. Expected loaded dict or DataFrame.")
