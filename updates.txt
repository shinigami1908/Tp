# after res = run_etl(...)
res = run_etl(write_combined_json=True, json_compress=False, write_feature_csv=True)

# res is a dict with keys like: 'loaded', 'combined_json', 'features_combined_df', ...
print("TOP-LEVEL RES KEYS:", list(res.keys()))

# The actual loaded tables live in res['loaded']
loaded = res.get("loaded")
if not isinstance(loaded, dict):
    print("DEBUG: res['loaded'] is missing or not a dict:", type(loaded))
else:
    print("\nDEBUG: loaded keys and row counts:")
    for k, df in loaded.items():
        if df is None:
            print(f"  {k:25} -> None")
        else:
            try:
                print(f"  {k:25} -> rows: {len(df):6}  columns: {list(df.columns)}")
            except Exception as e:
                print(f"  {k:25} -> (error reading df) {e}")

    # Also show a small sample of the employee_id values found across all tables:
    ids = set()
    for k, df in loaded.items():
        if df is None:
            continue
        if "employee_id" in df.columns:
            ids.update(df["employee_id"].astype(str).dropna().unique().tolist())
    print("\nDEBUG: total unique employee_ids found across tables:", len(ids))
    print("DEBUG: sample ids:", list(ids)[:10])
