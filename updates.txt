# src/features/feature_engineering.py
"""
Feature engineering (release_metrics fix + skip red_flag_dictionary rows)

Changes:
 - Normalize and coerce release_metrics columns early so they never become strings.
 - Skip red_flag_dictionary table for per-period aggregation (no red_flag_dictionary__rows).
 - Preserve text subarrays and normalize missing text cells to [].
 - Keeps all other behaviors (kpi__ prefixed KPIs, idp only in combined, column ordering).
"""
from typing import Dict, Tuple, List, Optional, Any
import pandas as pd
import numpy as np
from ..common.logger import get_logger
import datetime
import re

log = get_logger("feature_engineering")

NUMERIC_HINTS = {
    "jira_metrics": ["story_points", "committed", "completed", "spillover", "bugs", "story_points_committed", "story_points_completed"],
    "github_metrics": ["commits", "pull_requests", "reviews_done", "copilot", "copilot_suggestions"],
    "defects": ["ppm", "rollbacks", "escaped_to_prod"],
    "rto": ["required_days", "in_office_days", "compliant", "in_office_days"],
    "lms_completions": ["hours", "score"],
    "release_metrics": ["defect_rate_ppm", "rollbacks", "on_time"],
    "idp_goals": [],
}

CATEGORICAL_EXPAND = ["severity", "sentiment", "status", "type", "rating", "bias_type"]

TEXT_NAME_PATTERNS = re.compile(r"(concat|comment|message|summary|areas_of_focus|comments_concat|manager_comment)", flags=re.IGNORECASE)

COLUMN_TABLE_ORDER = [
    "manager_evaluations",
    "workday_checkins",
    "feedback_360",
    "defects",
    "release_metrics",
    "rto",
    "lms_completions",
    "recognition",
    "github_metrics",
    "jira_metrics",
]

SKIP_TABLES_FOR_PERIOD = {"employees", "employee", "employee.csv", "idp_goals", "red_flag_dictionary", "red_flag_dictionary.csv"}

def _bool_to_int_series(s: pd.Series) -> pd.Series:
    if s is None:
        return s
    def map_val(v):
        if v is None or (isinstance(v, float) and np.isnan(v)):
            return np.nan
        if isinstance(v, (bool, np.bool_)):
            return 1 if v else 0
        vs = str(v).strip().lower()
        if vs in ("true", "t", "yes", "y", "1"):
            return 1
        if vs in ("false", "f", "no", "n", "0"):
            return 0
        try:
            iv = int(float(vs))
            if iv in (0, 1):
                return iv
        except Exception:
            pass
        return np.nan
    return s.apply(map_val)

def _concat_texts(series: pd.Series, max_chars: int = 2000) -> str:
    try:
        vals = [str(x).strip() for x in series.dropna().astype(str)]
        seen = set()
        uniq = []
        for v in vals:
            if v and v not in seen:
                uniq.append(v)
                seen.add(v)
        joined = " || ".join(uniq)
        if len(joined) > max_chars:
            return joined[: max_chars - 3] + "..."
        return joined
    except Exception:
        return ""

def _normalize_loaded_keys(loaded: Dict[str, Any]) -> Dict[str, pd.DataFrame]:
    if not isinstance(loaded, dict):
        return loaded
    out = {}
    for k, v in loaded.items():
        nk = str(k).strip()
        nk = nk.replace(".csv", "").replace(".CSV", "")
        nk = nk.replace("-", "_").replace(" ", "_")
        while "__" in nk:
            nk = nk.replace("__", "_")
        nk = nk.lower()
        nk = nk.replace("feedback360", "feedback_360")
        nk = nk.replace("githubmetrics", "github_metrics")
        nk = nk.replace("lmscompletions", "lms_completions")
        nk = nk.replace("managerevaluations", "manager_evaluations")
        out[nk] = v
    return out

def _guess_numeric_cols(df: pd.DataFrame, hints: Optional[List[str]] = None) -> List[str]:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return []
    hints = hints or []
    cols = []
    for c in df.columns:
        if c in ("employee_id", "period_half"):
            continue
        lc = c.lower()
        if any(h in lc for h in hints):
            cols.append(c)
            continue
        if any(x in lc for x in ("on_time", "escaped_to_prod", "compliant", "in_office", "rollbacks")):
            cols.append(c)
            continue
        if pd.api.types.is_numeric_dtype(df[c]):
            cols.append(c)
            continue
        try:
            coerced = pd.to_numeric(df[c].astype(str).replace({"": np.nan, "nan": np.nan, "NA": np.nan, "N/A": np.nan}), errors="coerce")
            if coerced.notna().mean() >= 0.2:
                cols.append(c)
        except Exception:
            continue
    return cols

def _safe_numeric_sum_agg(df: pd.DataFrame, group_cols: List[str], numeric_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty or not numeric_cols:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for c in numeric_cols:
        if c not in d.columns:
            continue
        lname = c.lower()
        if lname.endswith("escaped_to_prod") or lname.endswith("on_time") or lname.endswith("compliant"):
            try:
                d[c] = _bool_to_int_series(d[c])
            except Exception:
                d[c] = pd.to_numeric(d[c], errors="coerce")
        else:
            try:
                d[c] = d[c].astype(str).str.strip().replace({"": None, "nan": None, "NA": None, "N/A": None})
                d[c] = pd.to_numeric(d[c], errors="coerce")
            except Exception:
                d[c] = pd.to_numeric(d[c], errors="coerce")
        d[c] = d[c].fillna(0)
    agg = d.groupby(group_cols, dropna=False)[[c for c in numeric_cols if c in d.columns]].sum().reset_index()
    return agg

def _expand_categorical_counts(df: pd.DataFrame, group_cols: List[str], cat_cols: List[str]) -> pd.DataFrame:
    present = [c for c in cat_cols if isinstance(df, pd.DataFrame) and c in df.columns]
    if not present:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for c in present:
        try:
            d[c] = d[c].astype(str).str.strip().fillna("unknown").replace({"nan": "unknown", "None": "unknown"})
        except Exception:
            d[c] = d[c].fillna("unknown").astype(str)
    dummies = pd.get_dummies(d[present].apply(lambda s: s.astype(str)))
    combined = pd.concat([d[group_cols].reset_index(drop=True), dummies.reset_index(drop=True)], axis=1)
    agg = combined.groupby(group_cols, dropna=False).sum().reset_index()
    for col in agg.columns:
        if col not in group_cols:
            try:
                agg[col] = agg[col].astype(int)
            except Exception:
                agg[col] = pd.to_numeric(agg[col], errors="coerce").fillna(0).astype(int)
    return agg

def _base_row_counts(df: pd.DataFrame, group_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols + ["_rows"])
    g = df.groupby(group_cols, dropna=False).size().reset_index().rename(columns={0: "_rows"})
    return g

def _merge_prefix(df: pd.DataFrame, prefix: str, group_cols: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols)
    out = df.copy()
    for c in df.columns:
        if c in group_cols:
            continue
        out = out.rename(columns={c: f"{prefix}__{c}"})
    return out

def _detect_red_flags(text: str, red_terms: List[str]) -> List[str]:
    if not text or not isinstance(text, str):
        return []
    t = text.lower()
    found = [rt for rt in red_terms if rt in t]
    return found

def _aggregate_text_subarray(df: pd.DataFrame, group_cols: List[str], text_col: str, item_key: str,
                             include_red_flag: bool, red_terms: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols + [text_col])
    working = df.copy()
    for gc in group_cols:
        if gc not in working.columns:
            working[gc] = "unknown"
    def make_list(sub):
        items = []
        for idx, row in sub.iterrows():
            txt = row.get(text_col)
            if not (isinstance(txt, str) and txt.strip()):
                continue
            entry = {item_key: str(txt)}
            if include_red_flag:
                entry["red_flag_trigger"] = _detect_red_flags(txt, red_terms or [])
            items.append(entry)
        return items
    grouped = working.groupby(group_cols, sort=False).apply(lambda g: make_list(g)).reset_index().rename(columns={0: text_col})
    return grouped

def _derive_safe_ratios(merged: pd.DataFrame) -> pd.DataFrame:
    df = merged.copy()
    def safe_div(a, b, scale=1.0):
        try:
            a = float(a) if a not in (None, "") else 0.0
            b = float(b) if b not in (None, "", 0, "0") else 0.0
            if b == 0:
                return 0.0
            return (a / b) * scale
        except Exception:
            return 0.0
    comp_col = df.get("jira_metrics__story_points_completed", pd.Series(0, index=df.index))
    comm_col = df.get("jira_metrics__story_points_committed", pd.Series(0, index=df.index))
    df["kpi__story_point_completion_ratio"] = [safe_div(a, b) for a, b in zip(comp_col.fillna(0), comm_col.fillna(0))]
    df["kpi__defect_escape_rate"] = df.apply(lambda r: safe_div(r.get("defects__escaped_to_prod", 0), r.get("defects___rows", 0)), axis=1)
    df["kpi__rto_compliance_rate"] = df.apply(lambda r: safe_div(r.get("rto__in_office_days", 0), r.get("rto__required_days", 0), scale=100.0), axis=1)
    df["kpi__rto_compliance_rate"] = df["kpi__rto_compliance_rate"].clip(upper=100.0)
    df["kpi__copilot_accept_ratio"] = df.apply(lambda r: safe_div(r.get("github_metrics__copilot_suggestions_accepted", 0), r.get("github_metrics__copilot_suggestions_total", 0)), axis=1)
    return df

def _process_text_fields_for_table(key: str, df: pd.DataFrame, group_cols: List[str], red_terms: List[str]) -> pd.DataFrame:
    if df is None or not isinstance(df, pd.DataFrame) or df.empty:
        return pd.DataFrame(columns=group_cols)
    d = df.copy()
    for gc in group_cols:
        if gc not in d.columns:
            d[gc] = "unknown"
    results = _base_row_counts(d, group_cols)
    results = _merge_prefix(results, key, group_cols)
    if key == "feedback_360":
        if "sentiment" in d.columns:
            sent_agg = _expand_categorical_counts(d, group_cols, ["sentiment"])
            sent_agg = _merge_prefix(sent_agg, key, group_cols)
            if not sent_agg.empty:
                results = results.merge(sent_agg, on=group_cols, how="left")
        if "comment" in d.columns:
            comments = _aggregate_text_subarray(d, group_cols, "comment", "comment", include_red_flag=False, red_terms=[])
            comments = _merge_prefix(comments, key, group_cols)
            if not comments.empty:
                results = results.merge(comments, on=group_cols, how="left")
    if key == "workday_checkins":
        if "type" in d.columns:
            type_agg = _expand_categorical_counts(d, group_cols, ["type"])
            type_agg = _merge_prefix(type_agg, key, group_cols)
            if not type_agg.empty:
                results = results.merge(type_agg, on=group_cols, how="left")
        if "sentiment" in d.columns:
            sent_agg = _expand_categorical_counts(d, group_cols, ["sentiment"])
            sent_agg = _merge_prefix(sent_agg, key, group_cols)
            if not sent_agg.empty:
                results = results.merge(sent_agg, on=group_cols, how="left")
        if "summary" in d.columns:
            summaries = _aggregate_text_subarray(d, group_cols, "summary", "summary", include_red_flag=True, red_terms=red_terms)
            summaries = _merge_prefix(summaries, key, group_cols)
            if not summaries.empty:
                results = results.merge(summaries, on=group_cols, how="left")
    if key == "manager_evaluations":
        if "manager_comment" in d.columns:
            mgr_comments = _aggregate_text_subarray(d, group_cols, "manager_comment", "comment", include_red_flag=True, red_terms=red_terms)
            mgr_comments = _merge_prefix(mgr_comments, key, group_cols)
            if not mgr_comments.empty:
                results = results.merge(mgr_comments, on=group_cols, how="left")
        if "areas_of_focus" in d.columns:
            areas = _aggregate_text_subarray(d, group_cols, "areas_of_focus", "area", include_red_flag=True, red_terms=red_terms)
            areas = _merge_prefix(areas, key, group_cols)
            if not areas.empty:
                results = results.merge(areas, on=group_cols, how="left")
    return results

def compute_kpis_from_loaded(loaded_raw: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if not isinstance(loaded_raw, dict):
        raise ValueError("compute_kpis_from_loaded expects a dict of DataFrames")
    loaded = _normalize_loaded_keys(loaded_raw)
    per_table_period: Dict[str, pd.DataFrame] = {}
    group_cols = ["employee_id", "period_half"]
    red_terms = []
    red_df = loaded.get("red_flag_dictionary")
    if isinstance(red_df, pd.DataFrame) and "red_flag_term" in red_df.columns:
        red_terms = [str(x).strip().lower() for x in red_df["red_flag_term"].dropna().unique().tolist()]
    for key, df in loaded.items():
        try:
            if key in SKIP_TABLES_FOR_PERIOD:
                per_table_period[key] = None
                continue
            if df is None or not isinstance(df, pd.DataFrame) or df.empty:
                per_table_period[key] = None
                continue
            working = df.copy()
            for gc in group_cols:
                if gc not in working.columns:
                    working[gc] = "unknown"
            # handle release_metrics: normalize column names to canonical ones before aggregation
            if key == "release_metrics":
                # build mapping from existing column names to canonical names
                col_map = {}
                for col in working.columns:
                    lc = col.lower().strip()
                    if "defect" in lc and ("ppm" in lc or "defect_rate" in lc):
                        col_map[col] = "defect_rate_ppm"
                    elif "on_time" in lc or "on time" in lc:
                        col_map[col] = "on_time"
                    elif "rollback" in lc or "rollbacks" in lc:
                        col_map[col] = "rollbacks"
                if col_map:
                    working = working.rename(columns=col_map)
                # coerce canonical columns
                for c in ("defect_rate_ppm", "on_time", "rollbacks"):
                    if c in working.columns:
                        if c == "on_time":
                            try:
                                working[c] = _bool_to_int_series(working[c])
                            except Exception:
                                working[c] = pd.to_numeric(working[c], errors="coerce")
                        else:
                            try:
                                working[c] = pd.to_numeric(working[c], errors="coerce")
                            except Exception:
                                working[c] = working[c]
            # text-special tables
            if key in ("feedback_360", "workday_checkins", "manager_evaluations"):
                txt_agg = _process_text_fields_for_table(key, working, group_cols, red_terms)
                per_table_period[key] = txt_agg
                continue
            # skip idp_goals already handled
            if key == "idp_goals":
                per_table_period[key] = None
                continue
            hints = NUMERIC_HINTS.get(key, [])
            numeric_candidates = _guess_numeric_cols(working, hints)
            for bool_like in ("escaped_to_prod", "on_time", "compliant"):
                if bool_like in working.columns and bool_like not in numeric_candidates:
                    numeric_candidates.append(bool_like)
            numeric_agg = _safe_numeric_sum_agg(working, group_cols, numeric_candidates)
            numeric_agg = _merge_prefix(numeric_agg, key, group_cols) if not numeric_agg.empty else pd.DataFrame(columns=group_cols)
            cat_agg = _expand_categorical_counts(working, group_cols, CATEGORICAL_EXPAND)
            cat_agg = _merge_prefix(cat_agg, key, group_cols) if not cat_agg.empty else pd.DataFrame(columns=group_cols)
            rows_agg = _base_row_counts(working, group_cols)
            rows_agg = _merge_prefix(rows_agg, key, group_cols)
            merged = rows_agg
            if not numeric_agg.empty:
                merged = merged.merge(numeric_agg, on=group_cols, how="left")
            if not cat_agg.empty:
                merged = merged.merge(cat_agg, on=group_cols, how="left")
            # ensure release_metrics prefixed columns numeric if exist
            if key == "release_metrics":
                for col in ("release_metrics__defect_rate_ppm", "release_metrics__on_time", "release_metrics__rollbacks"):
                    if col in merged.columns:
                        try:
                            merged[col] = pd.to_numeric(merged[col], errors="coerce").fillna(0)
                        except Exception:
                            merged[col] = merged[col].fillna(0)
            # post-merge coercion: preserve subarray/text columns, coerce numeric/boolean
            for col in merged.columns:
                if col in group_cols:
                    continue
                if TEXT_NAME_PATTERNS.search(col):
                    merged[col] = merged[col].where(merged[col].notna(), [])
                    continue
                lname = col.lower()
                if lname.endswith("__escaped_to_prod") or lname.endswith("__on_time") or lname.endswith("__compliant"):
                    try:
                        merged[col] = pd.to_numeric(merged[col], errors="coerce").fillna(0).astype(int)
                    except Exception:
                        merged[col] = merged[col].fillna(0)
                else:
                    try:
                        merged[col] = pd.to_numeric(merged[col], errors="coerce").fillna(0)
                    except Exception:
                        merged[col] = merged[col].fillna("")
            per_table_period[key] = merged
        except Exception as e:
            log.exception("Error processing table %s: %s", key, e)
            per_table_period[key] = None
    # Build canonical keys - exclude unknowns
    key_frames = []
    for k, agg_df in per_table_period.items():
        if agg_df is None or not isinstance(agg_df, pd.DataFrame) or agg_df.empty:
            continue
        if set(group_cols).issubset(set(agg_df.columns)):
            tmp = agg_df[group_cols].drop_duplicates()
            tmp = tmp[tmp["employee_id"].notna()]
            tmp = tmp[tmp["employee_id"].astype(str).str.strip().replace({"": np.nan}).notna()]
            tmp = tmp[tmp["employee_id"].astype(str).str.lower() != "unknown"]
            tmp = tmp[tmp["period_half"].astype(str).str.lower() != "unknown"]
            if not tmp.empty:
                key_frames.append(tmp)
    if key_frames:
        keys_df = pd.concat(key_frames, ignore_index=True).drop_duplicates().reset_index(drop=True)
    else:
        emp_df = loaded.get("employees")
        if isinstance(emp_df, pd.DataFrame) and "employee_id" in emp_df.columns:
            keys_df = emp_df[["employee_id"]].drop_duplicates().assign(period_half="NA")
        else:
            keys_df = pd.DataFrame(columns=group_cols)
    merged_all = keys_df.copy()
    # Merge per-table aggregates (skip idp_goals & red_flag_dictionary)
    for name, agg in per_table_period.items():
        if agg is None or not isinstance(agg, pd.DataFrame) or agg.empty:
            continue
        if not set(group_cols).issubset(set(agg.columns)):
            continue
        agg2 = agg.copy()
        agg2 = agg2[agg2["employee_id"].notna()]
        agg2 = agg2[agg2["employee_id"].astype(str).str.strip() != ""]
        agg2 = agg2[agg2["employee_id"].astype(str).str.lower() != "unknown"]
        merged_all = merged_all.merge(agg2, on=group_cols, how="left")
    # Normalize text subarray cells to [] and coerce numerics
    for col in merged_all.columns:
        if col in group_cols:
            continue
        if TEXT_NAME_PATTERNS.search(col):
            merged_all[col] = merged_all[col].apply(lambda x: x if isinstance(x, list) else ([] if pd.isna(x) or x == "" else x))
            continue
        try:
            coerced = pd.to_numeric(merged_all[col], errors="coerce")
            if coerced.notna().mean() >= 0.2:
                merged_all[col] = coerced.fillna(0)
            else:
                merged_all[col] = merged_all[col].fillna("").astype(str)
        except Exception:
            merged_all[col] = merged_all[col].fillna("").astype(str)
    features_by_period_df = merged_all
    # Derived KPIs at period level
    features_by_period_df = _derive_safe_ratios(features_by_period_df)
    # Build combined features
    if not features_by_period_df.empty:
        numeric_cols = [c for c in features_by_period_df.columns if c not in group_cols and pd.api.types.is_numeric_dtype(features_by_period_df[c])]
        features_combined_df = features_by_period_df.groupby("employee_id", dropna=False)[numeric_cols].sum().reset_index()
    else:
        features_combined_df = pd.DataFrame(columns=["employee_id"])
    # IDP combined-only
    idp_df = loaded.get("idp_goals")
    if isinstance(idp_df, pd.DataFrame) and not idp_df.empty:
        idp = idp_df.copy()
        if "employee_id" not in idp.columns:
            idp["employee_id"] = "unknown"
        if "status" not in idp.columns:
            idp["status"] = ""
        if "_idp_overdue_flag" not in idp.columns:
            def _is_overdue_row(r):
                td_raw = r.get("target_date")
                st = str(r.get("status", "")).strip().lower()
                try:
                    td = pd.to_datetime(td_raw, errors="coerce", infer_datetime_format=True)
                except Exception:
                    td = pd.NaT
                if pd.isna(td):
                    return 0
                if st == "completed":
                    return 0
                return int(td.date() < datetime.date.today())
            idp["_idp_overdue_flag"] = idp.apply(_is_overdue_row, axis=1)
        idp_group_cols = ["employee_id"]
        rows_agg = idp.groupby(idp_group_cols).size().reset_index().rename(columns={0: "idp_goals___rows"})
        status_dummies = pd.get_dummies(idp["status"].astype(str).str.strip().fillna("unknown")).add_prefix("idp_goals__")
        status_agg = pd.concat([idp[idp_group_cols].reset_index(drop=True), status_dummies.reset_index(drop=True)], axis=1).groupby(idp_group_cols).sum().reset_index()
        overdue_agg = idp.groupby(idp_group_cols)["_idp_overdue_flag"].sum().reset_index().rename(columns={"_idp_overdue_flag": "idp_goals__overdue_count"})
        idp_combined = rows_agg.merge(status_agg, on="employee_id", how="left").merge(overdue_agg, on="employee_id", how="left")
        for c in idp_combined.columns:
            if c == "employee_id":
                continue
            try:
                idp_combined[c] = pd.to_numeric(idp_combined[c], errors="coerce").fillna(0)
            except Exception:
                idp_combined[c] = idp_combined[c].fillna("")
        if "employee_id" in features_combined_df.columns:
            features_combined_df = features_combined_df.merge(idp_combined, on="employee_id", how="left")
        else:
            features_combined_df = features_combined_df.merge(idp_combined, on="employee_id", how="right")
    # Combine text subarrays across periods into combined
    text_cols = [c for c in features_by_period_df.columns if isinstance(c, str) and TEXT_NAME_PATTERNS.search(c)]
    for tc in text_cols:
        def flatten_lists(s):
            out = []
            for v in s:
                if isinstance(v, list):
                    out.extend(v)
                elif isinstance(v, str) and v.strip():
                    out.append({"text": v})
            return out
        combined_text = features_by_period_df.groupby("employee_id")[tc].apply(lambda s: flatten_lists(s.tolist())).reset_index().rename(columns={tc: tc})
        features_combined_df = features_combined_df.merge(combined_text, on="employee_id", how="left")
    # Ensure boolean-like aggregated columns are numeric ints in combined
    for col in features_combined_df.columns:
        lname = col.lower()
        if lname.endswith("escaped_to_prod") or lname.endswith("on_time") or lname.endswith("compliant"):
            try:
                features_combined_df[col] = pd.to_numeric(features_combined_df[col], errors="coerce").fillna(0).astype(int)
            except Exception:
                features_combined_df[col] = features_combined_df[col].fillna(0)
    # Recompute KPIs on combined and cap rto at 100
    features_combined_df = _derive_safe_ratios(features_combined_df)
    if "kpi__rto_compliance_rate" in features_combined_df.columns:
        features_combined_df["kpi__rto_compliance_rate"] = features_combined_df["kpi__rto_compliance_rate"].clip(upper=100.0)
    # Merge employee metadata (role, org, name)
    emp_df = loaded.get("employees")
    if isinstance(emp_df, pd.DataFrame) and "employee_id" in emp_df.columns:
        pick = ["employee_id"]
        if "name" in emp_df.columns:
            pick.append("name")
        if "role" in emp_df.columns:
            pick.append("role")
        if "org" in emp_df.columns:
            pick.append("org")
        emp_meta = emp_df[pick].drop_duplicates(subset=["employee_id"])
        features_combined_df = features_combined_df.merge(emp_meta, on="employee_id", how="left")
    # Column ordering enforcement
    def enforce_column_order(df: pd.DataFrame, is_period: bool) -> pd.DataFrame:
        cols = []
        cols.extend(["employee_id"])
        for m in ("name", "role", "org"):
            if m in df.columns:
                cols.append(m)
        if is_period:
            cols.append("period_half")
        for t in COLUMN_TABLE_ORDER:
            rows_col = f"{t}__rows"
            if rows_col in df.columns and rows_col not in cols:
                cols.append(rows_col)
            matching = sorted([c for c in df.columns if c.startswith(f"{t}__") and c not in cols])
            cols.extend(matching)
        if not is_period:
            idp_cols = sorted([c for c in df.columns if c.startswith("idp_goals__") or c.startswith("idp_goals___")])
            for c in idp_cols:
                if c not in cols:
                    cols.append(c)
        kpi_order = [
            "kpi__story_point_completion_ratio",
            "kpi__defect_escape_rate",
            "kpi__rto_compliance_rate",
            "kpi__copilot_accept_ratio",
        ]
        for k in kpi_order:
            if k in df.columns and k not in cols:
                cols.append(k)
        remaining = [c for c in df.columns if c not in cols]
        cols.extend(sorted(remaining))
        final_cols = [c for c in cols if c in df.columns]
        return df[final_cols]
    features_by_period_df = enforce_column_order(features_by_period_df, is_period=True)
    features_combined_df = enforce_column_order(features_combined_df, is_period=False)
    log.info("compute_kpis_from_loaded: produced features_by_period rows=%s, features_combined rows=%s",
             len(features_by_period_df), len(features_combined_df))
    return features_combined_df, features_by_period_df

def compute_kpis(joined_or_loaded: Any) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if isinstance(joined_or_loaded, dict) and "loaded" in joined_or_loaded and not any(isinstance(v, pd.DataFrame) for v in joined_or_loaded.values()):
        loaded = joined_or_loaded.get("loaded")
        if loaded is None:
            return compute_kpis_from_loaded(joined_or_loaded)
        return compute_kpis_from_loaded(loaded)
    if isinstance(joined_or_loaded, dict):
        return compute_kpis_from_loaded(joined_or_loaded)
    if isinstance(joined_or_loaded, pd.DataFrame):
        df = joined_or_loaded.copy()
        if "employee_id" not in df.columns:
            raise ValueError("compute_kpis received a DataFrame without 'employee_id' column")
        if "period_half" not in df.columns:
            if "period" in df.columns:
                df["period_half"] = df["period"].astype(str)
            else:
                df["period_half"] = "NA"
        group_cols = ["employee_id", "period_half"]
        numeric_candidates = []
        for c in df.columns:
            if c in ("employee_id", "period_half", "period", "name", "emp_role", "role"):
                continue
            try:
                sample = df[c].astype(str).str.strip().replace({"": None}).dropna().head(40)
                if sample.empty:
                    continue
                coerced = pd.to_numeric(sample, errors="coerce")
                if coerced.notna().mean() >= 0.6:
                    numeric_candidates.append(c)
            except Exception:
                continue
        if numeric_candidates:
            agg = df.groupby(group_cols, dropna=False)[numeric_candidates].sum().reset_index()
        else:
            agg = df.groupby(group_cols, dropna=False).size().reset_index().rename(columns={0: "_rows"})
        features_by_period_df = agg
        if not features_by_period_df.empty:
            numeric_cols = [c for c in features_by_period_df.columns if c not in group_cols]
            features_combined_df = features_by_period_df.groupby("employee_id", dropna=False)[numeric_cols].sum().reset_index()
            features_combined_df = _derive_safe_ratios(features_combined_df)
        else:
            features_combined_df = pd.DataFrame(columns=["employee_id"])
        return features_combined_df, features_by_period_df
    raise ValueError("compute_kpis: unsupported input type. Expected loaded dict or DataFrame.")
