# main.py

from src.etl.etl_pipeline import run_etl
from src.features.feature_engineering import compute_kpis
from src.llm.generator import (
    upload_all_report_cards,
    generate_manager_gpt_insights,
    generate_manager_console_report,
    get_current_manager_id,
)
from src.common.logger import get_logger

log = get_logger("main")


def main() -> None:
    # 1) ETL
    log.info("Running ETL...")
    etl_result = run_etl()  # dict with at least {"loaded": {...}} and combined_json, etc.

    # 2) Feature engineering / KPIs
    log.info("Computing features...")
    features_combined_df, features_by_period_df = compute_kpis(etl_result)
    log.info(
        "Features computed: combined rows=%s, by_period rows=%s",
        len(features_combined_df),
        len(features_by_period_df),
    )

    # 3) Build and upload full report cards JSON to Tachyon
    log.info("Building and uploading report cards JSON to Tachyon...")
    upload_resp = upload_all_report_cards()
    log.info("Upload response from Tachyon: %s", upload_resp)

    # 4) Generate insights for current manager (temporary hard-coded helper)
    manager_id = get_current_manager_id()
    log.info("Current manager_id (temporary helper) = %s", manager_id)

    # Optional: log a small local console snapshot (no GPT)
    log.info("Generating console report for manager_id=%s (no GPT)...", manager_id)
    generate_manager_console_report(manager_id=manager_id)

    # GPT insights (goes via TachyonChatManager, saves JSON to output/)
    log.info("Generating GPT insights for manager_id=%s...", manager_id)
    insights = generate_manager_gpt_insights(manager_id=manager_id)

    if isinstance(insights, dict) and insights.get("status") in {"error", "ok_raw"}:
        log.warning("GPT insights finished with status: %s", insights.get("status"))
    else:
        log.info("GPT insights generated successfully.")

    log.info("Pipeline finished. Outputs are in output/ directory.")


if __name__ == "__main__":
    main()
